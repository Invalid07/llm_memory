{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlHXxjUb+gAluc5Ib8ywbs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Invalid07/llm_memory/blob/main/llm_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vkgzkX96CoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instalin\n",
        "!pip install -q langchain_google_genai"
      ],
      "metadata": {
        "id": "K6a680wu-A5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KobsIIQm47Q8"
      },
      "outputs": [],
      "source": [
        "# importing\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "eU9m2qW5-8xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.4)"
      ],
      "metadata": {
        "id": "i0sXrN0r99Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory\n",
        "memory = ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "rQScDPVV-zao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# conversation\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "yxJMuv0g_IQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XETDowBD_ajX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDZPrd2cARBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.4)\n",
        "\n",
        "# memory\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "# prompt template\n",
        "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n",
        "Given the following conversation, summarize the conversation and then respond to the user's latest query.\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=template\n",
        "    )\n",
        "\n",
        "# conversation\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "output = conversation.predict(input=\"who musk ?\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXivO0X4ARFA",
        "outputId": "ff1b6c74-1c32-40ed-ec29-b5c0cee86520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15-616046204.py:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n",
            "Given the following conversation, summarize the conversation and then respond to the user's latest query.\n",
            "\n",
            "Human: who musk ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The conversation so far consists of a single question from the human: \"who musk?\".  The AI has not yet responded.\n",
            "\n",
            "\n",
            "To answer the question:  Assuming the user is asking about Elon Musk, he is a South African-born American entrepreneur and business magnate.  He is known for founding, co-founding, or leading several high-profile companies, including SpaceX (aerospace manufacturer and space transportation services), Tesla, Inc. (electric vehicle and clean energy company), and X Corp (formerly Twitter). He's also involved in Neuralink (neural implant technology) and The Boring Company (infrastructure and tunnel construction).  He's a highly influential and often controversial figure in the tech and business world, known for his ambitious goals and outspoken personality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tj3JredEAkkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MIdEJjW05lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJxASZxv05nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6usgSxqr05rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34c084c",
        "outputId": "f5864549-8c93-4f27-b11a-948133cb7ff6"
      },
      "source": [
        "!pip install -q langchain_google_genai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.0/1.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationSummaryBufferMemory**"
      ],
      "metadata": {
        "id": "zKqMdpdG1OuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.4)\n",
        "\n"
      ],
      "metadata": {
        "id": "feRIXMHVFEXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationSummaryBufferMemory\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEk0eVP50wmu",
        "outputId": "1a19f255-8a00-4503-8561-f7f6d5bc895b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-631206281.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conversation\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "Oy5KeUkn1YiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output\n",
        "\n",
        "output = conversation.predict(input=\" what is follow ai automation  ?\")\n",
        "print(output)\n",
        "\n",
        "# Continue the conversation until the user types 'exit' or 'quit'\n",
        "while True:\n",
        "  user_input = input(\"You: \")  # Get user input\n",
        "  if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "    break\n",
        "  else:\n",
        "    output = conversation.predict(input=user_input)\n",
        "    print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LvvrLsj1fyg",
        "outputId": "1bc506d9-1d6b-463d-f8d1-c79f47971380"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human asks about \"follow AI automation,\" a term the AI identifies as unfamiliar and possibly newly emerging, a misunderstanding, or a proprietary term.  The AI requests more context to understand the human's inquiry, suggesting the source of the term and its intended meaning.\n",
            "Human:  what is follow ai automation  ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm not familiar with the term \"follow AI automation.\"  It's possible this is a newly emerging term, a slight misunderstanding, or perhaps a proprietary term used within a specific company or industry.  To help me understand what you mean, could you please provide some more context?  For example:\n",
            "\n",
            "* **Where did you encounter this term?** (e.g., a book, article, website, conversation)  Knowing the source will help me understand the intended meaning.\n",
            "* **What is the context in which it was used?**  What was being discussed before or after the term was mentioned?\n",
            "* **What do you think it *might* mean?** Your interpretation, even if uncertain, can be helpful.\n",
            "\n",
            "The more information you can give me, the better chance I have of understanding what \"follow AI automation\" refers to.  Without more context, I can only speculate, and I'd rather give you a helpful and accurate answer based on a solid understanding of your question.\n",
            "You: give inlly follow\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human asks about \"follow AI automation,\" a term the AI doesn't recognize. The AI suggests the term might be new, a misunderstanding, or proprietary, and requests more context, including the source of the term, its surrounding context, and the human's interpretation of its meaning to accurately understand and respond to the inquiry.\n",
            "Human: give inlly follow\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm sorry, I don't understand what you mean by \"give inlly follow.\"  Could you please rephrase your request or provide more context?  The phrase is unfamiliar to me, and I need more information to understand what you're asking.  Perhaps you could tell me where you encountered this phrase, what it was referring to, and what you hoped to achieve by saying it.\n",
            "You: give only follow of automation\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human asks about \"follow AI automation,\" a term the AI doesn't recognize. The AI suggests the term might be new, a misunderstanding, or proprietary, and requests more context, including the source of the term, its surrounding context, and the human's interpretation of its meaning to accurately understand and respond to the inquiry.\n",
            "Human: give inlly follow\n",
            "AI: I'm sorry, I don't understand what you mean by \"give inlly follow.\"  Could you please rephrase your request or provide more context?  The phrase is unfamiliar to me, and I need more information to understand what you're asking.  Perhaps you could tell me where you encountered this phrase, what it was referring to, and what you hoped to achieve by saying it.\n",
            "Human: give only follow of automation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm still not sure what you mean by \"give only follow of automation.\"  This phrase isn't standard terminology in the field of artificial intelligence or automation.  It seems like there might be a typographical error or a misunderstanding.\n",
            "\n",
            "To help me understand, could you please clarify the following:\n",
            "\n",
            "* **Source:** Where did you encounter this phrase? (e.g., a book, article, conversation, website)  Providing the source will help me understand the context in which it was used.\n",
            "* **Surrounding Context:** What was being discussed before and after this phrase was used?  This will give me clues about its intended meaning.\n",
            "* **Your Interpretation:** What do *you* think \"give only follow of automation\" means?  Even if your interpretation is uncertain, sharing it will help me understand your perspective.\n",
            "\n",
            "For example, are you perhaps referring to:\n",
            "\n",
            "* **Following automated instructions?**  This would involve an AI or system following a pre-defined set of automated steps.\n",
            "* **Implementing only automated processes?** This might refer to a strategy of exclusively using automation for certain tasks.\n",
            "* **A specific type of automation system?**  There might be a niche or proprietary system with this name that I'm unfamiliar with.\n",
            "\n",
            "Please provide more details so I can assist you better.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "JCLok02q1sNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwR-dlkA3lBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5EpIyFh3lC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeWkZQYe3lGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entity memory\n",
        "it store information about these entities across conversation"
      ],
      "metadata": {
        "id": "GcovE9zA3tHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "AZJCE9ih4AhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}